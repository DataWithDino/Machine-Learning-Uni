{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Mathematische Grundlagen\n",
    "### 1.1 Hyperebene in der Linearen Algebra\n",
    "\n",
    "In der Linearen Algebra ist eine **Hyperebene** eine Verallgemeinerung von geometrischen Konzepten wie einer Linie oder einer Ebene. In einem n-dimensionalen Raum ist eine Hyperebene eine Subraumstruktur(Unterraum), die $n-1$ Dimensionen hat. Die mathematische Definition einer Hyperebene kann durch die folgende lineare Gleichung ausgedrückt werden:\n",
    "\n",
    "$$\n",
    "a_1x_1 + a_2x_2 + \\dots + a_nx_n = b\n",
    "$$\n",
    "\n",
    "Hierbei sind $a_1, a_2, \\dots, a_n$ und $b$ reelle Zahlen, und die $x_i$ sind die Koordinaten im n-dimensionalen Raum. Die Koeffizienten $a_i$ definieren die Orientierung der Hyperebene im Raum, und der Term $b$ positioniert die Hyperebene relativ zum Ursprung.\n",
    "\n",
    "### Anwendung im Maschinellen Lernen\n",
    "\n",
    "Im Bereich des maschinellen Lernens spielen Hyperebenen eine zentrale Rolle bei der Trennung von Datenpunkten in Klassifikationsalgorithmen. Ein bekanntes Beispiel ist der **Support Vector Machine (SVM)** Algorithmus, bei dem eine Hyperebene so gewählt wird, dass sie die optimale Trennung zwischen zwei Klassen von Datenpunkten bietet. Die Hyperebene wird so optimiert, dass sie den größtmöglichen Abstand (die Margin) zu den nächstliegenden Datenpunkten jeder Klasse hat. Die Formel für die Hyperebene in einem SVM-Modell lautet:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} \\cdot \\mathbf{x} + b = 0\n",
    "$$\n",
    "\n",
    "Hierbei ist $w$ ein Gewichtsvektor, der die Orientierung der Hyperebene definiert, und $b$ ist ein Bias-Term, der die Verschiebung der Hyperebene entlang des Vektors $w$ steuert. Die Datenpunkte, die direkt auf oder sehr nahe an der Margin liegen, werden als **Support Vektoren** bezeichnet.\n",
    "\n",
    "Ein weiteres Beispiel für die Anwendung von Hyperebenen im maschinellen Lernen ist das **lineare Regressionsmodell**, bei dem eine Hyperebene verwendet wird, um eine Beziehung zwischen mehreren unabhängigen Variablen und einer abhängigen Variablen zu modellieren. Hierbei versucht das Modell, den Fehler zwischen den durch die Hyperebene vorhergesagten Werten und den tatsächlichen Werten zu minimieren.\n",
    "\n",
    "Zusammenfassend ermöglichen Hyperebenen in der Linearen Algebra und im maschinellen Lernen die Darstellung und Analyse von Daten in hochdimensionalen Räumen und spielen eine Schlüsselrolle bei der Klassifikation und Regression in komplexen Datensätzen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Gradient in der Analysis\n",
    "\n",
    "Der Gradient ist ein Vektor, der aus den partiellen Ableitungen einer mehrdimensionalen Funktion besteht. Gegeben eine Funktion $f(x_1, x_2, \\dots, x_n)$, die von mehreren Variablen abhängt, ist der Gradient von $f$, notiert als $\\nabla f$, definiert als:\n",
    "\n",
    "$$\n",
    "\\nabla f = \\left(\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_n}\\right)\n",
    "$$\n",
    "\n",
    "Der Gradient zeigt in die Richtung des steilsten Anstiegs der Funktion. Das bedeutet, dass, wenn man sich in die Richtung des Gradienten von einem Punkt $x$ bewegt, der Wert der Funktion $f(x)$ am schnellsten zunimmt.\n",
    "\n",
    "## Anwendung des Gradienten im maschinellen Lernen\n",
    "\n",
    "Im Bereich des maschinellen Lernens ist der Gradient besonders wichtig für Optimierungsverfahren wie das Gradientenabstiegsverfahren (Gradient Descent). Diese Methode wird verwendet, um die Parameter eines Modells zu optimieren, in der Regel mit dem Ziel, eine Verlustfunktion zu minimieren. Die Verlustfunktion $L(\\theta)$ misst, wie gut das Modell die Daten nicht abbildet, wobei $\\theta$ die Parameter des Modells sind.\n",
    "\n",
    "Der Gradientenabstieg aktualisiert die Parameter in der entgegengesetzten Richtung des Gradienten der Verlustfunktion, um den Verlust zu reduzieren:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{neu}} = \\theta_{\\text{alt}} - \\eta \\nabla L(\\theta_{\\text{alt}})\n",
    "$$\n",
    "\n",
    "Hierbei ist $\\eta$ die Lernrate, eine positive Konstante, die bestimmt, wie groß die Schritte in Richtung des Minimums der Verlustfunktion sind.\n",
    "\n",
    "Durch wiederholte Anwendung dieses Updates versucht der Gradientenabstieg, das Minimum der Verlustfunktion zu finden, was einer optimalen Einstellung der Modellparameter entspricht. Dieser Prozess ist grundlegend für viele Lernalgorithmen in der künstlichen Intelligenz, einschließlich neuronaler Netze, logistischer Regression und vielen anderen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
