{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einleitung\n",
    "Bei den bisherigen Klassifikationsmethoden hatten wir angenommen, dass unsere Features reellwertig oder im Spezialfall diskret waren, also dass $X \\subseteq \\mathbb{R}^d$ bzw. $X \\subseteq \\mathbb{N}^d$. In beiden Fällen gibt es natürliche Distanzmetriken, wie z.B. die euklidische Norm $\\|x− x'\\|_2$. Mit Hilfe einer solchen Metrik kann z.B. bei KNN der bzw. die nächsten Nachbarn bestimmt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nominale Features\n",
    "Wir müssen jedoch auch den Fall betrachten, wenn $X$ nominal, das bedeutet diskret, jedoch ohne natürliche Distanzmetrik ist. Zum Beispiel wollen wir anhand der Features\n",
    "- Farbe $X_{\\text{Farbe}} = \\{\\text{rot, grün, gelb}\\}$\n",
    "- Form $X_{\\text{Form}} = \\{\\text{rund, dünn}\\}$\n",
    "- Größe $X_{\\text{Größe}} = \\{\\text{groß, mittel, klein}\\}$\n",
    "- Geschmack $X_{\\text{Geschmack}} = \\{\\text{süß, sauer}\\}$\n",
    "\n",
    "verschiedene Obstsorten beschreiben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Objekt $x \\in X$ wird demnach durch ein $d$-Tupel, in diesem Beispiel durch ein $4$-Tupel\n",
    "$x = (x_1, x_2, x_3, x_4) \\in X$\n",
    "mit\n",
    "$X = X_{\\text{Farbe}} \\times X_{\\text{Form}} \\times X_{\\text{Größe}} \\times X_{\\text{Geschmack}}$.\n",
    "So wäre ein Apfel etwa beschrieben durch $(\\text{rot, rund, mittel, süß})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klassifikation\n",
    "Bei der Klassifikation mit Entscheidungsbäumen befinden wir uns demnach in einem Szenario, bei welchem wir eine Funktion $f : X \\rightarrow Y$ lernen wollen, wobei $X$ reell, diskret oder auch nominal ist und $Y = \\{y_1, \\ldots, y_m\\}$ eine diskrete Menge von Klassen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei Entscheidungsbäumen klassifiziert man nun Objekte anhand nominaler Kriterien mit Hilfe von Fragensequenzen, wobei die nächste Frage abhängt von der Antwort auf die aktuelle Frage. Wichtig dabei ist, dass die Antwort auf jede Frage nominal ist wie z.B. generell $\\{\\text{ja, nein}\\}$ oder speziell $\\{\\text{rot, grün, gelb}\\}$. Solche Sequenzen von Fragen werden systematisch in einem Entscheidungsbaum repräsentiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufbau des Baums\n",
    "Die Knoten des Baums sind systematische Fragen und die Kanten die jeweiligen Antwortmöglichkeiten. Die Blätter des Baums sind die Klassen. Die Klassifikation beginnt mit der Frage in der Wurzel des Baums und endet in einem Blatt, also einer Klasse. Die Kanten, die einen Knoten verlassen, müssen\n",
    "- eindeutig und\n",
    "- erschöpfend\n",
    "sein, sodass immer genau einer Kante gefolgt wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entscheidungsbäume: Abbildung\n",
    "Abbildung 1: Entscheidungsbaum zur Klassifikation von Eigenschaften nach Obstsorten. Die Blätter des Baums sind die Klassen (Obstsorten) und sind blau markiert. Abbildung adaptiert von [DHS00]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretierbarkeit\n",
    "Eine Eigenschaft von Entscheidungsbäumen ist, dass sie sehr gut interpretierbar sind:\n",
    "- Die Klassifikation einzelner Datenpunkte $x \\in X$ kann vom Menschen nachvollzogen werden.\n",
    "- Die Klassen $y \\in Y$ selbst erhalten eine Beschreibung anhand von logischen Kriterien. Zum Beispiel:\n",
    "  - Apfel = $(\\text{Farbe} = \\text{grün} \\wedge \\text{Größe} = \\text{mittel})$\n",
    "  - $\\lor (\\text{Farbe} = \\text{rot} \\wedge \\text{Größe} = \\text{mittel})$\n",
    "  - $\\Rightarrow (\\text{Farbe} = \\text{grün} \\lor \\text{Farbe} = \\text{rot}) \\wedge \\text{Größe} = \\text{mittel}$\n",
    "- Entscheidungsbäume können daher auch durch explizites Vorwissen ergänzt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART Framework\n",
    "Wie wird nun ausgehend von\n",
    "- einer Menge von Trainingsdaten $D \\subseteq X \\times Y$ und\n",
    "- einer Menge von Entscheidungsfragen\n",
    "ein Entscheidungsbaum gelernt? Ein Entscheidungsbaum teilt sukzessive die Menge $D$ in immer kleinere Teilmengen. Idealerweise endet jeder Pfad in einer reinen Menge, d.h. einer Menge $F \\subseteq D$ für die gilt, dass alle Labels $y$ mit $(x, y) \\in F$ gleich sind. Dies ist üblicherweise nicht der Fall und es muss geregelt werden, ob in solchen weitere Aufteilungen erfolgen sollen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allgemeiner Lernalgorithmus\n",
    "Allgemeiner Lernalgorithmus für Entscheidungsbäume mit Trainingsdaten $D$ und Menge an Fragen $Q$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allgemeiner Lernalgorithmus\n",
    "Allgemeiner Lernalgorithmus für Entscheidungsbäume mit Trainingsdaten $D$ und Menge an Fragen $Q$:\n",
    "\n",
    "Algorithm 1 dtree(D, Q)\n",
    "1: if stop_criteria(D) then\n",
    "2: return LEAF(compute_class(D))\n",
    "3: else\n",
    "4: for each $q \\in Q$ do\n",
    "5: $S_q = \\text{split}(D, q)$\n",
    "6: $\\Delta i_q = \\text{compute_improvement}(D,S)$\n",
    "7: end for\n",
    "8: $b = \\text{argmax}_{q | \\Delta i_q>0} \\Delta i_q$\n",
    "9: return NODE(b,dtree($S^{(b)}_1, Q \\backslash b$), dtree($S^{(b)}_2, Q \\backslash b$), . . . )\n",
    "10: end if"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART Methodik\n",
    "Das CART (Classification And Regression Tree) Framework bietet eine allgemeine Methodik um verschiedenste Arten von Entscheidungsbäumen zu generieren anhand von sechs grundlegenden Fragestellungen:\n",
    "1. Wieviele Entscheidungsmöglichkeiten und damit Aufteilungen gibt es pro Knoten?\n",
    "2. Welche Eigenschaften werden in einem Knoten getestet?\n",
    "3. Wann soll ein Knoten zu einem Blatt werden?\n",
    "4. Wie wird ein zu großer Baum gestutzt?\n",
    "5. Wie soll einem unreinen Blatt eine Klasse zugeordnet werden?\n",
    "6. Wie wird mit unvollständigen Daten umgegangen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufteilungen\n",
    "Jede Entscheidung ist mit einem Split (Aufteilung) der Trainingsdaten verbunden. Die Anzahl der Splits kann frei gewählt werden und auch innerhalb eines Baums variieren. Bereits zwei Splits reichen im Allgemeinen aus, d.h. binäre Entscheidungsbäume sind universell. Die Entscheidung beeinflusst potentiell die Performance der Methodik und auch die Wahl der Eigenschaften."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entscheidungsbäume: Binärer Baum\n",
    "Abbildung 2: Ein binärer Entscheidungsbaum zur Klassifikation von Eigenschaften nach Obstsorten. Die Blätter des Baums sind die Klassen (Obstsorten) und sind blau markiert. Positive Entscheidungen (ja) sind grün und negative Entscheidungen (nein) sind rot markiert. Abbildung adaptiert von [DHS00]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fragenauswahl und Knotenunreinheit\n",
    "Hauptziel der Erstellung eines Entscheidungsbaums ist die Einfachheit, d.h. ein Baum mit möglichst wenigen Kanten und Knoten. Wir suchen daher für jeden Knoten die Frage, welche die resultierenden Datenmengen so rein wie möglich macht. Wir nähern uns formal dem Konzept der Reinheit durch das Gegenteil, der Unreinheit (Impurity). Wir bezeichnen mit $i(N)$ die Unreinheit in Knoten $N$ und es sollte gelten, dass\n",
    "- $i(N) = 0$, falls alle Daten in Knoten $N$ die gleiche Klasse haben\n",
    "- $i(N)$ groß, wenn alle Kategorien gleich häufig vertreten sind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unreinheit\n",
    "Das bekannteste Maß für Unreinheit oder auch Unordnung ist die (Shannon-)Entropie definiert als\n",
    "$i(N) = -\\sum_{j=1}^m P(y_j) \\log_2 P(y_j)$\n",
    "wobei $P(y_j)$ die relative Häufigkeit der Klasse $y_j$ innerhalb der Trainingsdaten an Knoten $N$ bezeichnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beispiele für Unreinheit\n",
    "- Schwarze und weiße Kugeln - Unrein: $p_\\circ = p_\\bullet = \\frac{4}{8} \\Rightarrow i(N) = -2 \\cdot 0.5 \\log_2 0.5 = 1$\n",
    "- Schwarze und weiße Kugeln - Reiner: $p_\\circ = \\frac{1}{8}, p_\\bullet = \\frac{7}{8} \\Rightarrow i(N) = -\\frac{1}{8} \\log_2 \\frac{1}{8} - \\frac{7}{8} \\log_2 \\frac{7}{8} \\approx 0.54$\n",
    "- Schwarze und weiße Kugeln - Rein: $p_\\circ = 0, p_\\bullet = 1 \\Rightarrow i(N) = -1 \\log_2 1 =0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Gini Unreinheit und Missclassification Impurity\n",
    "Ein weiteres häufiges Maß ist die Gini Unreinheit definiert als\n",
    "$i(N) = \\sum_{i \\neq j} P(y_i)P(y_j) = \\frac{1}{2} \\left(1 - \\sum_{j=1}^m P^2(y_j) \\right)$\n",
    "und die Missclassification Impurity definiert als\n",
    "$i(N) = 1 - \\max_j P(y_j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots der Unreinheitsmaße\n",
    "Abbildung 3: Plots der Unreinheitsmaße Entropie (orange), Gini Impurity (grün) und Missclassification Impurity (blau) in Abhängigkeit von der relativen (binären) Klassenzugehörigkeit $p = P(y_1) = 1 - P(y_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split-Heuristik\n",
    "Wir betrachten der Einfachheit halber nur den Fall eines (bisher partiell bis zu einem Knoten $N$ erstellten) binären Baums und wollen wissen, welche Frage an diesem Knoten an die übrigen Testdaten gestellt werden sollte. Eine Heuristik in diesem Fall ist die Frage, welche den Rückgang bzgl. der Unreinheit definiert als\n",
    "$\\Delta i(N) = i(N) - P_P i(N_P) - P_N i(N_N)$\n",
    "minimiert. $N_P$ und $N_N$ sind die positiven bzw. negativen Nachfolgeknoten von $N$ und $P_P = 1 - P_N$ ist der Anteil der Datenpunkte, die dem positiven Knoten zugeordnet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nominale und reellwertige Features\n",
    "Hinweise:\n",
    "- Bei nominalen Features muss meist ein vollständiger Vergleich aller möglichen Fragen pro Knoten in allen Dimensionen durchgeführt werden. Im Beispiel der Obstklassifikation etwa $Größe = \\text{klein}, \\ldots, Größe = \\text{groß}, Farbe = \\text{rot}, \\ldots, Farbe = \\text{gelb}, Geschmack = \\text{sauer}, \\ldots$\n",
    "- Bei diskreten und reellen Features werden oft Vergleiche der Art $x_i \\leq c$ mit $c \\in \\mathbb{R}$ verwendet. Generell beschränkt sich der Suchraum für die Konstanten $c$ meist auf tatsächlich in den Trainingsdaten vorkommende Werte von $x_i$ oder gewichtete Mittel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entscheidungsbaum-Beispiel\n",
    "Wir wollen nun einen Entscheidungsbaum mit Hilfe der bisherigen Ideen (Entropie als Unreinheitsmaß) aufbauen.\n",
    "Abbildung 4: Beispiel: Klassifikation von Süßigkeiten.\n",
    "$i(N) = -\\frac{1}{2} \\log_2 \\frac{1}{2} - \\frac{1}{2} \\log_2 \\frac{1}{2} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auswahl der ersten Frage\n",
    "Frage an der Wurzel $x_1 = \\text{rot}$:\n",
    "$i(L) = -\\frac{1}{3} \\log_2 \\frac{1}{3} - \\frac{2}{3} \\log_2 \\frac{2}{3} \\approx 0.9183$\n",
    "$i(R) = -\\frac{2}{3} \\log_2 \\frac{2}{3} - \\frac{1}{3} \\log_2 \\frac{1}{3} \\approx 0.9183$\n",
    "$\\Delta i(N) \\approx 1 - \\frac{1}{2} \\cdot 0.9183 - \\frac{1}{2} \\cdot 0.9183 \\approx 0.0817$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
