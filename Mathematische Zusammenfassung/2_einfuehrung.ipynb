{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Einführung\n",
    "## 2.1 Überwachtes Lernen\n",
    "\n",
    "Beim überwachten Lernen versuchen wir eine Funktion\n",
    "\n",
    "$f:X→Y$\n",
    "\n",
    "zu nden, welche den Zusammenhang zwischen den potentiell mehrdimensionalen Mengen X und Y möglichst gut repräsentiert, denn meistens werden wir eine perfekte Abbildung aufgrund von statistischen Effekten nicht erreichen. Dabei gibt es zwei Arten von Fehlern:\n",
    "\n",
    "- reduzierbar z.B. durch eine bessere Funktion $f$\n",
    "- nicht reduzierbar z.B. aufgrund von Messfehlern in den Daten\n",
    "\n",
    "## Modell?\n",
    "Wir nennen eine Repräsentation von $f$ mathematisch aber auch als Datenstruktur im Computer Modell.\n",
    "\n",
    "Die Dimensionen von:\n",
    "- $X$ werden Eingabevariablen, Prädiktoren, unabhängige Variablen oder Features\n",
    "- $X$ werden Eingabevariablen, Prädiktoren, unabhängige Variablen oder Features\n",
    "  \n",
    "genannt.\n",
    "\n",
    "### Ziele\n",
    "Grundsätzlich gibt es beim überwachten Lernen zwei grobe Zielsetzungen zwischen denen meist abgewogen werden muss:\n",
    "- Vorhersage: Gewünscht ist eine möglichst gute Vorhersage\n",
    "$y = f (x)$ wobei die Funktionsweise von f im Extremfall eine Blackbox sein kann.\n",
    "- Inferenz: Hier steht die Interpretierbarkeit von $f$ im Vordergrund, z.B. Aussagen welche Prädiktoren für welchen Response relevant sind oder auch welcher Zusammenhang (linear, quadratisch, etc.) genau besteht.\n",
    "\n",
    "### Herangehensweise\n",
    "Auch für die Herangehensweise gibt es im Groÿen und Ganzen zwei Möglichkeiten:\n",
    "- Parametrische Methoden: Hier wird zunächst eine Annahme bzgl. einer parametrisierten Struktur von $f$ gemacht und diese Parameter werden schlieÿlich mit Hilfe von Daten bestimmt.\n",
    "- Nicht-parametrische Methoden: Es wird keine Annahme bzgl. der Struktur von $f$ gemacht und es wird versucht $f$ möglichst direkt mit Hilfe von Daten zu definieren.\n",
    "\n",
    "### Kategorien\n",
    "- Ist $Y$ eine diskrete Menge,dann handelt es sich um ein Klassifikationsproblem. Bei der Klassifikation sind wir an qualitativen Aussagen interessiert. Die einzelnen Objekte der diskreten Menge $C1,...,C_k$ werdenKlassenoderKategorien genannt.\n",
    "- Ist Y eine kontinuierliche Menge, dann handelt es sich um ein Regressionsproblem. Bei der Regression sind wir an quantitativen Aussagen interessiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Unüberwachtes Lernen\n",
    "\n",
    "Beim unüberwachten Lernen versucht man ohne Zuhilfenahme von gelabelten Daten einen Mehrwert zu erhalten. Das Ziel ist daher ausgehend von einer Menge von Daten mehr über die BeschaFFenheit von $X$ herauszubekommen, um dieses Wissen dann direkt oder indirekt anwenden zu \n",
    "können.\n",
    "\n",
    "### Beispiele\n",
    "- Lernen der Verteilung von X z.B. bei Sprachmodellen (Welche Wörter folgen auf ein bestimmtes Wort oder einen Satz).\n",
    "- Dimensionsreduktion zur Verbesserung von überwachten Lernverfahren,z.B. $ X=R^{10}$ statt $X=R^{1004}$ für $f:X →Y$\n",
    "- Finden von Ähnlichkeitsstrukturen durch Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Datenaufbereitung\n",
    "Bevor tatsächlich ein ML Modell erstellt und trainiert wird, müssen die entsprechenden Daten vorverarbeitet werden. Dazu gehören grundsätzlich drei Schritte\n",
    "- Auswahl\n",
    "- Aufbereitung\n",
    "- Transformation\n",
    "\n",
    "der Daten. Oftmals muss auch aufgrund neuer Erkenntnisse zwischen den Schritten hin und her gewechselt werden.\n",
    "\n",
    "### Auswahl\n",
    "Auswahl: Nicht immer sind mehr Daten auch wirklich besser, d.h. es sollte darauf geachtet werden, dass nur für den Anwendungszweck relevante Daten verwendet werden, um die Rechen- und Speicheranforderungen im Rahmen zu halten. Auch die Leistung des Systems könnte u.U. unter zu vielen bzw. den falschen Daten leiden auch natürlich auch unter zu wenig.\n",
    "\n",
    "### Aufbereitung\n",
    "- Definition eines geeigneten Formats (Tabellen, Big Data Formate wie Parquet, CSV, Bilder, etc.) und Umwandlung der Daten\n",
    "- Bereinigung, d.h. Entfernung von unvollständigen oder ungültigen Daten oder aufgrund von rechtlichen Bestimmungen (Datenschutz)\n",
    "- Unterauswahl der Daten (lange Laufzeit, groÿer Speicheraufwand). Hier muss auf eine repräsentative Auswahl (Zeit, Ort, Gruppen, etc.) geachtet werden, um keinen systematischen Fehler einzuführen.\n",
    "\n",
    "### Transformation\n",
    "- Skalierung: Features in den geeigneten Wertebereich für ML Methode bringen, z.B. auf Wertebereiche [0, 1] oder [−1, 1]. Auch eine Normierung auf Mittelwert 0 und Standardabweichung 1 kann notwendig sein.\n",
    "- Zerlegung in sinnvolle Features, z.B. Extraktion der Zeit und des Fehlercodes aus Logfile-Eintägen\n",
    "- Aggregation mehrerer Features, z.B. Gesamtzahl der Aktienverkäufe an einem Tag statt jede Einzeltransaktion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
